# inventory_app.py (Google Drive API - .xlsx ì§ì ‘ ë¡œë”© ë²„ì „ - ìš”ì¼ í‘œì‹œ ë° ìš©ì–´ ë³€ê²½ + ë””ë²„ê¹… ì½”ë“œ ì¶”ê°€)

import streamlit as st

# --- 1. í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ” Streamlit ëª…ë ¹) ---
st.set_page_config(page_title="ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ", layout="wide", initial_sidebar_state="expanded")

import pandas as pd
import datetime
from dateutil.relativedelta import relativedelta
import os # í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€
import traceback
import plotly.express as px
import json # JSON ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€
import io # ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€

# Google Drive API ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload

# --- í•œêµ­ì–´ ìš”ì¼ ë¦¬ìŠ¤íŠ¸ ---
KOREAN_DAYS = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']

# --- Google API ì¸ì¦ ë° Drive ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ---
drive_service = None
SERVICE_ACCOUNT_LOADED = False


# --- !!! ì¤‘ìš”: ë””ë²„ê¹… ì½”ë“œ ì‹œì‘ !!! ---
# Streamlit Cloud í™˜ê²½ì¸ì§€, Secretsê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.
# ë¬¸ì œê°€ í•´ê²°ëœ í›„ì—ëŠ” ì´ ë””ë²„ê¹… ê´€ë ¨ st.write, st.error, st.text ë¬¸êµ¬ë“¤ì„ ì‚­ì œí•˜ê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.

st.write(f"í™˜ê²½ ë³€ìˆ˜ IS_STREAMLIT_CLOUD ê°’: {os.getenv('IS_STREAMLIT_CLOUD')}") # Streamlit Cloudì—ì„œ ì„¤ì •í•˜ëŠ” í™˜ê²½ ë³€ìˆ˜ ê°’ í™•ì¸
IS_CLOUD_ENVIRONMENT = os.getenv('IS_STREAMLIT_CLOUD') == 'true'
st.write(f"IS_CLOUD_ENVIRONMENT ë³€ìˆ˜ í‰ê°€ ê²°ê³¼: {IS_CLOUD_ENVIRONMENT}") # ìœ„ í™˜ê²½ ë³€ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í´ë¼ìš°ë“œ í™˜ê²½ì¸ì§€ ì—¬ë¶€

if IS_CLOUD_ENVIRONMENT:
    st.write("í´ë¼ìš°ë“œ í™˜ê²½ìœ¼ë¡œ íŒë‹¨ë¨. st.secretsì—ì„œ ì¸ì¦ ì •ë³´ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
    if "google_creds_json" not in st.secrets:
        st.error("ì˜¤ë¥˜: st.secretsì— 'google_creds_json' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤! Streamlit Cloud ëŒ€ì‹œë³´ë“œì—ì„œ Secrets ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        # st.write("í˜„ì¬ st.secretsì— ìˆëŠ” í‚¤ ëª©ë¡ (ë¯¼ê°í•œ ê°’ì€ ì œì™¸):", st.secrets.to_dict().keys()) # ì–´ë–¤ í‚¤ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸ìš©
    else:
        try:
            creds_json_str = st.secrets["google_creds_json"]
            st.success("'google_creds_json' í‚¤ë¥¼ st.secretsì—ì„œ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
            # st.text("ê°€ì ¸ì˜¨ JSON ë¬¸ìì—´ì˜ ì¼ë¶€ (ì• 100ì): " + creds_json_str[:100] + "...") # ì‹¤ì œ ê°’ì˜ ì¼ë¶€ë¥¼ ë³´ê³  ì‹¶ì„ ë•Œ (ì£¼ì˜!)

            creds_dict = json.loads(creds_json_str) # JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            st.success("st.secretsì—ì„œ ê°€ì ¸ì˜¨ JSON ë¬¸ìì—´ì„ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤.")

            scopes = ['https://www.googleapis.com/auth/drive.readonly']
            creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
            drive_service = build('drive', 'v3', credentials=creds)
            SERVICE_ACCOUNT_LOADED = True
            st.success("Google Drive ì„œë¹„ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. SERVICE_ACCOUNT_LOADED = True")
        except json.JSONDecodeError as e_json:
            st.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: st.secretsì˜ 'google_creds_json' ë‚´ìš©ì„ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e_json}")
            st.error("Secretsì— ì…ë ¥ëœ JSON ë¬¸ìì—´ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€, íŠ¹íˆ ì¤„ë°”ê¿ˆ, ë”°ì˜´í‘œ ë“±ì´ ì •í™•í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.text("ë¬¸ì œê°€ ë˜ëŠ” JSON ë¬¸ìì—´ì˜ ì¼ë¶€ (ì• 200ì): " + creds_json_str[:200] + "...") # ë¬¸ì œ ë¶€ë¶„ í™•ì¸ìš©
        except KeyError: # ì´ ê²½ìš°ëŠ” "google_creds_json" not in st.secrets ì—ì„œ ì´ë¯¸ ê±¸ëŸ¬ì§€ì§€ë§Œ, ë§Œì•½ì„ ìœ„í•´ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
            st.error("KeyError: st.secretsì—ì„œ 'google_creds_json' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            if 'sidebar_error_displayed' not in st.session_state: # ê¸°ì¡´ ì˜¤ë¥˜ ë©”ì‹œì§€ ìœ ì§€
                st.sidebar.error("ì˜¤ë¥˜: í´ë¼ìš°ë“œ Secretsì— 'google_creds_json' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
                st.session_state.sidebar_error_displayed = True
        except Exception as e_secrets:
            st.error(f"í´ë¼ìš°ë“œ Secrets ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸ ë°œìƒ: {e_secrets}")
            if 'sidebar_error_displayed' not in st.session_state: # ê¸°ì¡´ ì˜¤ë¥˜ ë©”ì‹œì§€ ìœ ì§€
                st.sidebar.error(f"ì˜¤ë¥˜: í´ë¼ìš°ë“œ Secrets ì¸ì¦ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e_secrets}")
                st.session_state.sidebar_error_displayed = True
else:
    st.write("í´ë¼ìš°ë“œ í™˜ê²½ì´ ì•„ë‹˜ (ë˜ëŠ” IS_CLOUD_ENVIRONMENTê°€ False). ë¡œì»¬ íŒŒì¼ì—ì„œ ì¸ì¦ ì •ë³´ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
# --- !!! ì¤‘ìš”: ë””ë²„ê¹… ì½”ë“œ ë !!! ---


# ë¡œì»¬ í™˜ê²½ì¼ ê²½ìš° (SERVICE_ACCOUNT_LOADEDê°€ Falseì¼ ë•Œë§Œ ì‹¤í–‰)
if not SERVICE_ACCOUNT_LOADED:
    SERVICE_ACCOUNT_FILE = r"C:\Users\kmeat 1f\Documents\googleaiy\clear-shadow-444503-q4-88934382a4ce.json" # ì‚¬ìš©ì ì‹¤ì œ ê²½ë¡œ
    st.write(f"ë¡œì»¬ ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ê²½ë¡œ: {SERVICE_ACCOUNT_FILE}") # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ í™•ì¸ìš© (í´ë¼ìš°ë“œì—ì„œëŠ” ì´ ë¶€ë¶„ì´ ì‹¤í–‰ë˜ë©´ ì•ˆë¨)
    try:
        if os.path.exists(SERVICE_ACCOUNT_FILE):
            scopes = ['https://www.googleapis.com/auth/drive.readonly']
            creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=scopes)
            drive_service = build('drive', 'v3', credentials=creds)
            SERVICE_ACCOUNT_LOADED = True
            st.info(f"ë¡œì»¬ íŒŒì¼({SERVICE_ACCOUNT_FILE})ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì¸ì¦ ì •ë³´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤. SERVICE_ACCOUNT_LOADED = True")
        else:
            if not IS_CLOUD_ENVIRONMENT: # í´ë¼ìš°ë“œ í™˜ê²½ì´ ì•„ë‹ ë•Œë§Œ ì´ ê²½ê³ ë¥¼ í‘œì‹œ (ë””ë²„ê¹… ë©”ì‹œì§€ì™€ ì¤‘ë³µë  ìˆ˜ ìˆìŒ)
                st.warning(f"ê²½ê³ : ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SERVICE_ACCOUNT_FILE}. Google Drive ì—°ë™ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                if 'sidebar_error_displayed' not in st.session_state:
                    st.session_state.sidebar_error_displayed = True
    except Exception as e_local:
        if 'sidebar_error_displayed' not in st.session_state:
            st.sidebar.warning(f"ê²½ê³ : ë¡œì»¬ í‚¤ íŒŒì¼ ì¸ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({SERVICE_ACCOUNT_FILE}): {e_local}. Google Drive ì—°ë™ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            st.session_state.sidebar_error_displayed = True


if not SERVICE_ACCOUNT_LOADED or drive_service is None:
    st.error("### ì¤‘ìš”: Google Drive API ì¸ì¦ ì‹¤íŒ¨! ###")
    st.error("Google Drive ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•± ìƒë‹¨ì˜ ë””ë²„ê¹… ë©”ì‹œì§€ë‚˜ Streamlit Cloud ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”.")
    if 'critical_auth_error_displayed' not in st.session_state:
        st.session_state.critical_auth_error_displayed = True
    st.stop() # ì¸ì¦ ì•ˆë˜ë©´ ì•± ì¤‘ë‹¨

# --- Google Drive íŒŒì¼ ID ì •ì˜ ---
SM_FILE_ID = "1tRljdvOpp4fITaVEXvoL9mNveNg2qt4p"
PURCHASE_FILE_ID = "1AgKl29yQ80sTDszLql6oBnd9FnLWf8oR" # ì…ê³  íŒŒì¼ ID
SALES_FILE_ID = "1h-V7kIoInXgGLll7YBW5V_uZdF3Q1PdY"   # ì¶œê³  íŒŒì¼ ID
ERP_FILE_ID = "1Lbtwenw8LcDaj94_J4kKTjoWQY7PEAZs"
ADDRESS_UPDATE_FILE_ID = "1t1ORfuuHfW3VZ0yXTiIaaBgHzYF8MDwd"


# --- ë°ì´í„° ì²˜ë¦¬ìš© ìƒìˆ˜ ---
SM_QTY_COL_TREND = 'ì”ëŸ‰(ë°•ìŠ¤)'
SM_WGT_COL_TREND = 'ì”ëŸ‰(Kg)'

REPORT_LOCATION_MAP_TREND = {'ì‹ ê°ˆëƒ‰ë™': 'ì‹ ê°ˆ', 'ì„ ì™•CH4ì¸µ': 'ì„ ì™•', 'ì‹ ê°ˆê¹€í˜•ì œ': 'ê¹€í˜•ì œ', 'ì‹ ê°ˆìƒì´í’ˆ/ì‘ì—…': 'ìƒì´í’ˆ', 'ì¼€ì´ë¯¸íŠ¸ìŠ¤í† ì–´': 'ìŠ¤í† ì–´'}
TARGET_SM_LOCATIONS_FOR_TREND = ['ì‹ ê°ˆëƒ‰ë™', 'ì„ ì™•CH4ì¸µ', 'ì‹ ê°ˆê¹€í˜•ì œ', 'ì‹ ê°ˆìƒì´í’ˆ/ì‘ì—…', 'ì¼€ì´ë¯¸íŠ¸ìŠ¤í† ì–´']
REPORT_ROW_ORDER_TREND = ['ì‹ ê°ˆ', 'ì„ ì™•', 'ê¹€í˜•ì œ', 'ìƒì´í’ˆ', 'ìŠ¤í† ì–´']
# ì…ê³  ê´€ë ¨ ìƒìˆ˜
PURCHASE_DATE_COL = 'ë§¤ì…ì¼ì'; PURCHASE_CODE_COL = 'ì½”ë“œ'; PURCHASE_CUSTOMER_COL = 'ê±°ë˜ì²˜ëª…' # ì›ë³¸ íŒŒì¼ ì»¬ëŸ¼ëª… ìœ ì§€
PURCHASE_PROD_CODE_COL = 'ìƒí’ˆì½”ë“œ'; PURCHASE_PROD_NAME_COL = 'ìƒ í’ˆ ëª…'; PURCHASE_LOCATION_COL = 'ì§€ ì  ëª…'
PURCHASE_QTY_BOX_COL = 'Box'; PURCHASE_QTY_KG_COL = 'Kg'
PURCHASE_LOG_SHEET_NAME = 'p-list' # ì›ë³¸ íŒŒì¼ ì‹œíŠ¸ëª… ìœ ì§€
# ì¶œê³  ê´€ë ¨ ìƒìˆ˜
SALES_DATE_COL = 'ë§¤ì¶œì¼ì'; SALES_PROD_CODE_COL = 'ìƒí’ˆì½”ë“œ'; SALES_PROD_NAME_COL = 'ìƒ  í’ˆ  ëª…' # ì›ë³¸ íŒŒì¼ ì»¬ëŸ¼ëª… ìœ ì§€
SALES_QTY_BOX_COL = 'ìˆ˜ëŸ‰(Box)'; SALES_QTY_KG_COL = 'ìˆ˜ëŸ‰(Kg)'; SALES_LOCATION_COL = 'ì§€ì ëª…'
SALES_LOG_SHEET_NAME = 's-list' # ì›ë³¸ íŒŒì¼ ì‹œíŠ¸ëª… ìœ ì§€

CURRENT_STOCK_PROD_CODE_COL = 'ìƒí’ˆì½”ë“œ'; CURRENT_STOCK_PROD_NAME_COL = 'ìƒí’ˆëª…'
CURRENT_STOCK_QTY_COL = SM_QTY_COL_TREND
CURRENT_STOCK_WGT_COL = SM_WGT_COL_TREND
CURRENT_STOCK_LOCATION_COL = 'ì§€ì ëª…'
SUMMARY_TABLE_LOCATIONS = ['ì‹ ê°ˆëƒ‰ë™', 'ì„ ì™•CH4ì¸µ', 'ì‹ ê°ˆê¹€í˜•ì œ', 'ì‹ ê°ˆìƒì´í’ˆ/ì‘ì—…', 'ì¼€ì´ë¯¸íŠ¸ìŠ¤í† ì–´']


# --- Google Drive íŒŒì¼ ë‹¤ìš´ë¡œë“œ í—¬í¼ í•¨ìˆ˜ ---
@st.cache_data(ttl=300)
def download_excel_from_drive(file_id, file_name_for_error_msg=""):
    if drive_service is None: # ì´ í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ê¸° ì „ì— drive_serviceê°€ ì´ˆê¸°í™”ë˜ì–´ì•¼ í•¨
        st.error(f"ì˜¤ë¥˜: íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„ ì¤‘ Google Drive ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ ({file_name_for_error_msg}). ì•± ìƒë‹¨ ì¸ì¦ ë¶€ë¶„ì„ í™•ì¸í•˜ì„¸ìš”.")
        if 'drive_service_none_in_download' not in st.session_state:
            st.session_state.drive_service_none_in_download = True
        return None
    try:
        request = drive_service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request)
        done = False
        while done is False:
            status, done = downloader.next_chunk()
        fh.seek(0)
        return fh
    except HttpError as error:
        session_key_download_error = f"download_error_displayed_{file_id}"
        if session_key_download_error not in st.session_state :
            st.sidebar.warning(f"ê²½ê³ : íŒŒì¼(ID: {file_id}, ì´ë¦„: {file_name_for_error_msg}) ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {error.resp.status} - {error._get_reason()}. íŒŒì¼ ê³µìœ  ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            st.session_state[session_key_download_error] = True
        return None
    except Exception as e:
        session_key_general_error = f"general_error_displayed_{file_id}"
        if session_key_general_error not in st.session_state:
            st.sidebar.warning(f"ê²½ê³ : íŒŒì¼(ID: {file_id}, ì´ë¦„: {file_name_for_error_msg}) ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            st.session_state[session_key_general_error] = True
        return None

# --- ë°ì´í„° ë¡œë”© í•¨ìˆ˜ (Drive API ì‚¬ìš©) ---
@st.cache_data(ttl=300)
def get_all_available_sheet_dates_from_excel(file_id, file_name_for_error_msg="SMì¬ê³ í˜„í™©.xlsx"):
    fh = download_excel_from_drive(file_id, file_name_for_error_msg)
    if fh is None: return []
    try:
        xls = pd.ExcelFile(fh)
        sheet_names = xls.sheet_names
        valid_dates = []
        for name in sheet_names:
            try:
                dt_obj = datetime.datetime.strptime(name, "%Y%m%d").date()
                valid_dates.append(dt_obj)
            except ValueError:
                continue
        valid_dates.sort(reverse=True)
        return valid_dates
    except Exception as e:
        st.sidebar.warning(f"ê²½ê³ : '{file_name_for_error_msg}' ì—‘ì…€ íŒŒì¼ ì‹œíŠ¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

@st.cache_data(ttl=300)
def load_sm_data_from_excel(file_id, date_strings_yyyymmdd_list, file_name_for_error_msg="SMì¬ê³ í˜„í™©.xlsx"):
    if not date_strings_yyyymmdd_list: return None
    fh = download_excel_from_drive(file_id, file_name_for_error_msg)
    if fh is None: return None

    all_data = []
    try:
        xls = pd.ExcelFile(fh)
        available_sheets = xls.sheet_names
        for date_str in date_strings_yyyymmdd_list:
            if date_str in available_sheets:
                try:
                    df_sheet = pd.read_excel(xls, sheet_name=date_str, header=0)
                    df_sheet.dropna(how='all', inplace=True)
                    if df_sheet.empty: continue

                    required_cols = ['ì§€ì ëª…', 'ìƒí’ˆì½”ë“œ', SM_QTY_COL_TREND, SM_WGT_COL_TREND]
                    if not all(col in df_sheet.columns for col in required_cols):
                        missing = [col for col in required_cols if col not in df_sheet.columns]
                        st.sidebar.warning(f"ê²½ê³ : '{file_name_for_error_msg}' íŒŒì¼ì˜ '{date_str}' ì‹œíŠ¸ì— í•„ìˆ˜ ì»¬ëŸ¼ {missing} ì¤‘ ì¼ë¶€ê°€ ëˆ„ë½ë˜ì–´ í•´ë‹¹ ì‹œíŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                    df_sheet_copy = df_sheet.copy()
                    df_sheet_copy['ë‚ ì§œ'] = pd.to_datetime(date_str, format='%Y%m%d')
                    df_processed_sheet = df_sheet_copy[['ë‚ ì§œ', 'ì§€ì ëª…', 'ìƒí’ˆì½”ë“œ', SM_QTY_COL_TREND, SM_WGT_COL_TREND]].copy()

                    for col in [SM_QTY_COL_TREND, SM_WGT_COL_TREND]:
                        df_processed_sheet[col] = pd.to_numeric(df_processed_sheet[col], errors='coerce').fillna(0)

                    df_processed_sheet['ì§€ì ëª…'] = df_processed_sheet['ì§€ì ëª…'].astype(str).str.strip()
                    df_processed_sheet['ë‚ ì§œ'] = pd.to_datetime(df_processed_sheet['ë‚ ì§œ']).dt.normalize()
                    all_data.append(df_processed_sheet)
                except Exception as e_sheet:
                    st.sidebar.warning(f"ê²½ê³ : '{file_name_for_error_msg}' íŒŒì¼ì˜ ì‹œíŠ¸ '{date_str}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e_sheet}")
                    continue
        if not all_data: return None
        return pd.concat(all_data, ignore_index=True)
    except Exception as e_main:
        st.sidebar.error(f"ì˜¤ë¥˜: '{file_name_for_error_msg}' ì—‘ì…€ íŒŒì¼ ë¡œë”© ì¤‘ ì£¼ìš” ì˜¤ë¥˜ ë°œìƒ: {e_main}")
        return None

@st.cache_data(ttl=300)
def get_latest_date_from_log(file_id, sheet_name, date_col, file_name_for_error_msg=""): # file_name_for_error_msgì— "ì…ê³ ë‚´ì—­.xlsx" ë˜ëŠ” "ì¶œê³ ë‚´ì—­.xlsx" ì „ë‹¬
    fh = download_excel_from_drive(file_id, file_name_for_error_msg)
    if fh is None: return None
    try:
        df = pd.read_excel(fh, sheet_name=sheet_name, header=0)
        df.dropna(subset=[date_col], how='all', inplace=True)
        if df.empty or date_col not in df.columns: return None

        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        df.dropna(subset=[date_col], inplace=True)
        if df.empty: return None

        return df[date_col].max().date()
    except Exception as e:
        st.sidebar.warning(f"ê²½ê³ : '{file_name_for_error_msg}' ({sheet_name} ì‹œíŠ¸) ìµœì‹  ë‚ ì§œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

@st.cache_data(ttl=300)
def load_daily_log_data_for_period_from_excel(file_id, sheet_name, date_col, location_col, qty_box_col, qty_kg_col, start_date, end_date, is_purchase_log=False, file_name_for_error_msg=""): # file_name_for_error_msgì— "ì…ê³ ë‚´ì—­.xlsx" ë˜ëŠ” "ì¶œê³ ë‚´ì—­.xlsx" ì „ë‹¬
    fh = download_excel_from_drive(file_id, file_name_for_error_msg)
    if fh is None: return pd.DataFrame()
    try:
        df = pd.read_excel(fh, sheet_name=sheet_name, header=0)
        df.dropna(how='all', inplace=True)
        if df.empty: return pd.DataFrame()

        if is_purchase_log:
            ffill_cols = [date_col, location_col, PURCHASE_CODE_COL, PURCHASE_CUSTOMER_COL]
            for col_to_ffill in ffill_cols:
                if col_to_ffill in df.columns:
                    df[col_to_ffill] = df[col_to_ffill].ffill()
                elif col_to_ffill == date_col or col_to_ffill == location_col:
                    st.sidebar.warning(f"ê²½ê³ : '{file_name_for_error_msg}' ({sheet_name}) ì…ê³  ë¡œê·¸ì— í•„ìˆ˜ ffill ì»¬ëŸ¼({col_to_ffill}) ëˆ„ë½.")
                    return pd.DataFrame()

        required_cols_log = [date_col, location_col, qty_box_col, qty_kg_col]
        if not all(col in df.columns for col in required_cols_log):
            missing_log_cols = [col for col in required_cols_log if col not in df.columns]
            st.sidebar.warning(f"ê²½ê³ : '{file_name_for_error_msg}' ({sheet_name})ì— í•„ìˆ˜ ì»¬ëŸ¼ {missing_log_cols} ëˆ„ë½.")
            return pd.DataFrame()

        df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.normalize()
        df.dropna(subset=[date_col], inplace=True)
        if df.empty: return pd.DataFrame()

        mask = (df[date_col].dt.date >= start_date) & (df[date_col].dt.date <= end_date)
        df_period = df.loc[mask].copy()

        if df_period.empty: return pd.DataFrame()

        for col in [qty_box_col, qty_kg_col]:
            df_period[col] = pd.to_numeric(df_period[col], errors='coerce').fillna(0)
        df_period[location_col] = df_period[location_col].astype(str).str.strip()

        daily_summary = df_period.groupby([df_period[date_col].dt.date, location_col]).agg(
            TotalQtyBox=(qty_box_col, 'sum'),
            TotalQtyKg=(qty_kg_col, 'sum')
        ).reset_index()
        daily_summary.rename(columns={date_col: 'ë‚ ì§œ'}, inplace=True)
        return daily_summary
    except Exception as e:
        st.sidebar.error(f"ì˜¤ë¥˜: '{file_name_for_error_msg}' ({sheet_name} ì‹œíŠ¸) ì¼ë³„ ê¸°ê°„ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()


@st.cache_data(ttl=300)
def load_log_data_for_period_from_excel(file_id, sheet_name, date_col, qty_kg_col, location_col, start_date, end_date, is_purchase_log=False, file_name_for_error_msg=""): # file_name_for_error_msgì— "ì…ê³ ë‚´ì—­.xlsx" ë˜ëŠ” "ì¶œê³ ë‚´ì—­.xlsx" ì „ë‹¬
    fh = download_excel_from_drive(file_id, file_name_for_error_msg)
    if fh is None: return pd.DataFrame()
    try:
        df = pd.read_excel(fh, sheet_name=sheet_name, header=0)
        df.dropna(how='all', inplace=True)
        if df.empty: return pd.DataFrame()

        if is_purchase_log:
            ffill_cols = [date_col, location_col, PURCHASE_CODE_COL, PURCHASE_CUSTOMER_COL]
            for col_to_ffill in ffill_cols:
                if col_to_ffill in df.columns: df[col_to_ffill] = df[col_to_ffill].ffill()
                elif col_to_ffill == date_col or col_to_ffill == location_col:
                    st.sidebar.warning(f"ê²½ê³ : '{file_name_for_error_msg}' ({sheet_name}) ì…ê³  ë¡œê·¸(ì›”ë³„)ì— í•„ìˆ˜ ffill ì»¬ëŸ¼({col_to_ffill}) ëˆ„ë½.")
                    return pd.DataFrame()

        if date_col not in df.columns or qty_kg_col not in df.columns:
            st.sidebar.warning(f"ê²½ê³ : '{file_name_for_error_msg}' ({sheet_name})ì— í•„ìˆ˜ ì»¬ëŸ¼ ({date_col} ë˜ëŠ” {qty_kg_col}) ëˆ„ë½.")
            return pd.DataFrame()

        df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.normalize()
        df.dropna(subset=[date_col], inplace=True)
        df[qty_kg_col] = pd.to_numeric(df[qty_kg_col], errors='coerce').fillna(0)

        mask = (df[date_col].dt.date >= start_date) & (df[date_col].dt.date <= end_date)
        df_period = df.loc[mask].copy()
        if df_period.empty: return pd.DataFrame()

        df_period['ì›”'] = df_period[date_col].dt.strftime('%Y-%m')
        monthly_sum = df_period.groupby('ì›”')[qty_kg_col].sum().reset_index()
        monthly_sum.rename(columns={qty_kg_col: 'ì¤‘ëŸ‰(Kg)'}, inplace=True)
        return monthly_sum
    except Exception as e:
        st.sidebar.error(f"ì˜¤ë¥˜: '{file_name_for_error_msg}' ({sheet_name} ì‹œíŠ¸) ê¸°ê°„ ë°ì´í„°(ì›”ë³„) ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

# --- í˜ì´ì§€ ë Œë”ë§ í•¨ìˆ˜ ì •ì˜ ---
def render_daily_trend_page_layout():
    now_time = datetime.datetime.now()
    current_time_str = now_time.strftime("%Y-%m-%d %H:%M:%S")
    st.markdown(f"<h1 style='text-align: center; margin-bottom: 0.1rem;'>ğŸ“Š ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ</h1>", unsafe_allow_html=True)
    st.markdown(f"<p style='text-align: center; margin-top: 0.1rem; font-size: 0.9em;'>í˜„ì¬ ì‹œê°„: {current_time_str}</p>", unsafe_allow_html=True)
    st.markdown("---", unsafe_allow_html=True)

    all_available_dates_desc = get_all_available_sheet_dates_from_excel(SM_FILE_ID, "SMì¬ê³ í˜„í™©.xlsx")
    dates_for_report = []

    if not all_available_dates_desc:
        st.warning("ê²½ê³ : 'SMì¬ê³ í˜„í™©.xlsx' íŒŒì¼ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ í˜•ì‹ì˜ ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ë‚´ìš© ë° ì‹œíŠ¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        today = datetime.date.today()
        latest_anchor_date = next((dt for dt in all_available_dates_desc if dt <= today), None)

        if latest_anchor_date is None:
            st.warning(f"ê²½ê³ : ì˜¤ëŠ˜({today.strftime('%Y-%m-%d')}) ë˜ëŠ” ê·¸ ì´ì „ ë‚ ì§œì— ëŒ€í•œ ë°ì´í„°ë¥¼ 'SMì¬ê³ í˜„í™©.xlsx'ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°€ì¥ ìµœê·¼ ë°ì´í„°ë¡œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            latest_anchor_date = all_available_dates_desc[0]

        start_index = all_available_dates_desc.index(latest_anchor_date)
        end_index = min(start_index + 7, len(all_available_dates_desc))
        dates_for_report = all_available_dates_desc[start_index:end_index][:7]

        if dates_for_report:
            dates_for_report.sort()
            st.info(f"ë¶„ì„ ê¸°ê°„ ({len(dates_for_report)}ì¼ ë°ì´í„°): {dates_for_report[0].strftime('%Y-%m-%d')} ~ {dates_for_report[-1].strftime('%Y-%m-%d')}")
        else:
            st.warning("ê²½ê³ : ë¦¬í¬íŠ¸ì— ì‚¬ìš©í•  ë‚ ì§œë¥¼ ì„ ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 'SMì¬ê³ í˜„í™©.xlsx' íŒŒì¼ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    report_dates_pd = pd.to_datetime(dates_for_report).normalize() if dates_for_report else pd.DatetimeIndex([])
    report_date_str_list_yyyymmdd = [d.strftime("%Y%m%d") for d in dates_for_report]

    df_sm_trend_raw = None
    if report_date_str_list_yyyymmdd:
        df_sm_trend_raw = load_sm_data_from_excel(SM_FILE_ID, report_date_str_list_yyyymmdd, "SMì¬ê³ í˜„í™©.xlsx")

    daily_location_summary = None
    if df_sm_trend_raw is not None and not df_sm_trend_raw.empty:
        df_sm_trend_filtered = df_sm_trend_raw[df_sm_trend_raw['ì§€ì ëª…'].isin(TARGET_SM_LOCATIONS_FOR_TREND)].copy()
        if not df_sm_trend_filtered.empty:
            df_sm_trend_filtered['ì°½ê³ ëª…'] = df_sm_trend_filtered['ì§€ì ëª…'].map(REPORT_LOCATION_MAP_TREND)
            daily_location_summary = df_sm_trend_filtered.groupby(['ë‚ ì§œ', 'ì°½ê³ ëª…'])[[SM_QTY_COL_TREND, SM_WGT_COL_TREND]].sum().reset_index()

    title_style = "<h3 style='margin-bottom:0.2rem; margin-top:0.5rem; font-size:1.25rem;'>"

    # --- ì²« ë²ˆì§¸ í–‰: 3ê°œ í•­ëª© ---
    row1_cols = st.columns(3)

    with row1_cols[0]:
        st.markdown(f"{title_style}1. ì¼ë³„ ì¬ê³ ëŸ‰({SM_QTY_COL_TREND}) ì¶”ì´</h3>", unsafe_allow_html=True)
        if daily_location_summary is not None and not daily_location_summary.empty and not report_dates_pd.empty:
            try:
                daily_location_summary['ë‚ ì§œ'] = pd.to_datetime(daily_location_summary['ë‚ ì§œ']).dt.normalize()
                chart_pivot_qty_raw = daily_location_summary.pivot_table(index='ë‚ ì§œ', columns='ì°½ê³ ëª…', values=SM_QTY_COL_TREND)
                chart_pivot_qty_final = chart_pivot_qty_raw.reindex(index=report_dates_pd, columns=REPORT_ROW_ORDER_TREND).fillna(0)
                st.line_chart(chart_pivot_qty_final, use_container_width=True, height=250)
            except Exception as e_chart1:
                st.write(f"ì¬ê³ ëŸ‰ ì¶”ì´ ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜.")
                st.line_chart(pd.DataFrame(index=report_dates_pd, columns=REPORT_ROW_ORDER_TREND).fillna(0), use_container_width=True, height=250)
        elif dates_for_report:
            st.write("í‘œì‹œí•  ê·¸ë˜í”„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.line_chart(pd.DataFrame(index=report_dates_pd, columns=REPORT_ROW_ORDER_TREND).fillna(0), use_container_width=True, height=250)
        else:
            st.write("ë°ì´í„° ë¡œë“œ ë¶ˆê°€ ë˜ëŠ” ë¶„ì„ ê¸°ê°„ ì—†ìŒ")


    with row1_cols[1]:
        st.markdown(f"{title_style}2. ì¬ê³  ë¹„ì¤‘ ({SM_QTY_COL_TREND})</h3>", unsafe_allow_html=True)
        if daily_location_summary is not None and not daily_location_summary.empty and not report_dates_pd.empty:
            latest_report_date_ts = report_dates_pd[-1]
            df_latest_day_stock = daily_location_summary[daily_location_summary['ë‚ ì§œ'] == latest_report_date_ts]
            if not df_latest_day_stock.empty and df_latest_day_stock[SM_QTY_COL_TREND].sum() > 0:
                fig_qty = px.pie(df_latest_day_stock, names='ì°½ê³ ëª…', values=SM_QTY_COL_TREND, hole=.4, title=f"{latest_report_date_ts.strftime('%m/%d')} (ë°•ìŠ¤)")
                fig_qty.update_traces(textposition='inside', textinfo='percent+label', pull=[0.05 if qty > 0 else 0 for qty in df_latest_day_stock[SM_QTY_COL_TREND]])
                fig_qty.update_layout(showlegend=False, title_x=0.5, margin=dict(t=40, b=20, l=20, r=20), height=280)
                st.plotly_chart(fig_qty, use_container_width=True)
            elif dates_for_report: st.write(f"{latest_report_date_ts.strftime('%m/%d')} ë°ì´í„° ì—†ìŒ")
            else: st.write("ë°ì´í„° ì—†ìŒ")
        elif dates_for_report: st.write("ìµœì‹ ì¼ì ë°ì´í„° ì—†ìŒ")
        else: st.write("ë°ì´í„° ë¡œë“œ ë¶ˆê°€ ë˜ëŠ” ë¶„ì„ ê¸°ê°„ ì—†ìŒ")

    with row1_cols[2]:
        st.markdown(f"{title_style}3. ì¬ê³  ë¹„ì¤‘ ({SM_WGT_COL_TREND})</h3>", unsafe_allow_html=True)
        if daily_location_summary is not None and not daily_location_summary.empty and not report_dates_pd.empty:
            latest_report_date_ts = report_dates_pd[-1]
            df_latest_day_stock_wgt = daily_location_summary[daily_location_summary['ë‚ ì§œ'] == latest_report_date_ts]
            if not df_latest_day_stock_wgt.empty and df_latest_day_stock_wgt[SM_WGT_COL_TREND].sum() > 0:
                fig_wgt = px.pie(df_latest_day_stock_wgt, names='ì°½ê³ ëª…', values=SM_WGT_COL_TREND, hole=.4, title=f"{latest_report_date_ts.strftime('%m/%d')} (Kg)")
                fig_wgt.update_traces(textposition='inside', textinfo='percent+label', pull=[0.05 if wgt > 0 else 0 for wgt in df_latest_day_stock_wgt[SM_WGT_COL_TREND]])
                fig_wgt.update_layout(showlegend=False, title_x=0.5, margin=dict(t=40, b=20, l=20, r=20), height=280)
                st.plotly_chart(fig_wgt, use_container_width=True)
            elif dates_for_report: st.write(f"{latest_report_date_ts.strftime('%m/%d')} ë°ì´í„° ì—†ìŒ")
            else: st.write("ë°ì´í„° ì—†ìŒ")
        elif dates_for_report: st.write("ìµœì‹ ì¼ì ë°ì´í„° ì—†ìŒ")
        else: st.write("ë°ì´í„° ë¡œë“œ ë¶ˆê°€ ë˜ëŠ” ë¶„ì„ ê¸°ê°„ ì—†ìŒ")

    st.markdown("---")

    # --- ë‘ ë²ˆì§¸ í–‰ (ì¹¸ 4): ì¼ë³„ ì°½ê³  ì¬ê³ ëŸ‰ í‘œ (ê°€ë¡œ ì „ì²´) ---
    st.markdown(f"{title_style}4. ì¼ë³„ ì°½ê³  ì¬ê³ ëŸ‰ ({SM_QTY_COL_TREND}/{SM_WGT_COL_TREND})</h3>", unsafe_allow_html=True); st.caption("í‘œê°€ ê¸¸ ê²½ìš° ìŠ¤í¬ë¡¤í•˜ì„¸ìš”.")
    if daily_location_summary is not None and not daily_location_summary.empty and not report_dates_pd.empty:
        try:
            table_pivot_qty = daily_location_summary.pivot_table(index='ì°½ê³ ëª…', columns='ë‚ ì§œ', values=SM_QTY_COL_TREND, fill_value=0).reindex(index=REPORT_ROW_ORDER_TREND, columns=report_dates_pd, fill_value=0)
            table_pivot_wgt = daily_location_summary.pivot_table(index='ì°½ê³ ëª…', columns='ë‚ ì§œ', values=SM_WGT_COL_TREND, fill_value=0).reindex(index=REPORT_ROW_ORDER_TREND, columns=report_dates_pd, fill_value=0)

            qty_diff = pd.DataFrame(index=table_pivot_qty.index, columns=table_pivot_qty.columns); daily_qty_totals = table_pivot_qty.sum(axis=0)
            daily_wgt_totals = table_pivot_wgt.sum(axis=0); total_qty_diff = pd.Series(dtype='float64', index=daily_qty_totals.index)

            if len(table_pivot_qty.columns) > 1: qty_diff = table_pivot_qty.diff(axis=1)
            if len(daily_qty_totals.index) > 1: total_qty_diff = daily_qty_totals.diff()

            combined_table = pd.DataFrame(index=table_pivot_qty.index, columns=table_pivot_qty.columns, dtype=object)
            for date_col_ts in combined_table.columns: # date_col_tsëŠ” Timestamp
                qty_series = table_pivot_qty[date_col_ts]; wgt_series = table_pivot_wgt[date_col_ts]; diff_series = qty_diff.get(date_col_ts)
                cell_strings = []
                for warehouse in combined_table.index:
                    qty_val = qty_series.get(warehouse, 0); wgt_val = wgt_series.get(warehouse, 0); diff_val = diff_series.get(warehouse, None) if diff_series is not None else None
                    base_string = f"{qty_val:,.0f} / {wgt_val:,.1f} Kg"; indicator = ""
                    if qty_val == 0 and wgt_val == 0: cell_strings.append("-")
                    else:
                        if pd.notnull(diff_val) and len(table_pivot_qty.columns) > 1:
                            if diff_val > 0.01: indicator = "ğŸ”º "
                            elif diff_val < -0.01: indicator = "â–¼ "
                        cell_strings.append(f"{indicator}{base_string}")
                combined_table[date_col_ts] = cell_strings

            total_row_data = {}
            for date_col_ts in table_pivot_qty.columns: # date_col_tsëŠ” Timestamp
                total_qty_val = daily_qty_totals.get(date_col_ts, 0); total_wgt_val = daily_wgt_totals.get(date_col_ts, 0); total_diff_val = total_qty_diff.get(date_col_ts, None)
                base_total_string = f"{total_qty_val:,.0f} / {total_wgt_val:,.1f} Kg"; total_indicator = ""
                if total_qty_val == 0 and total_wgt_val == 0: total_row_data[date_col_ts] = "-"
                else:
                    if pd.notnull(total_diff_val) and len(daily_qty_totals.index) > 1:
                        if total_diff_val > 0.01: total_indicator = "ğŸ”º "
                        elif total_diff_val < -0.01: total_indicator = "â–¼ "
                    total_row_data[date_col_ts] = f"{total_indicator}{base_total_string}"
            combined_table.loc['í•©ê³„'] = pd.Series(total_row_data)

            # ë‚ ì§œ ì»¬ëŸ¼ì— ìš”ì¼ ì¶”ê°€ (MM/DD(ìš”ì¼) í˜•ì‹)
            combined_table.columns = [ts.strftime('%m/%d') + f"({KOREAN_DAYS[ts.weekday()]})" for ts in combined_table.columns]

            combined_table_display = combined_table.reindex(REPORT_ROW_ORDER_TREND + ['í•©ê³„'])
            st.dataframe(combined_table_display.reset_index().rename(columns={'index': 'ì°½ê³ ëª…'}), hide_index=True, use_container_width=True, height=300)
        except Exception as e_table:
            st.error(f"í‘œ ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e_table}")
            # traceback.print_exc() # ê°œë°œ ì‹œ ìƒì„¸ ì˜¤ë¥˜ í™•ì¸
    elif dates_for_report:
        st.write("í‘œì‹œí•  í…Œì´ë¸” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        if not report_dates_pd.empty:
            empty_table_cols = [ts.strftime('%m/%d') + f"({KOREAN_DAYS[ts.weekday()]})" for ts in report_dates_pd]
            empty_table_data = {col_name: ['-'] * (len(REPORT_ROW_ORDER_TREND) + 1) for col_name in empty_table_cols}
            empty_table_df = pd.DataFrame(empty_table_data, index=REPORT_ROW_ORDER_TREND + ['í•©ê³„']); empty_table_df.index.name = 'ì°½ê³ ëª…'
            st.dataframe(empty_table_df.reset_index(), hide_index=True, use_container_width=True, height=300)
    else:
        st.write("ë°ì´í„° ë¡œë“œ ë¶ˆê°€ ë˜ëŠ” ë¶„ì„ ê¸°ê°„ ì—†ìŒ")

    st.markdown("---")

    # --- ì„¸ ë²ˆì§¸ í–‰ (ì¹¸ 5): ìµœê·¼ 7ì¼ ì¼ë³„ ì…ê³ /ì¶œê³  í˜„í™© (ê°€ë¡œ ì „ì²´) ---
    st.markdown(f"{title_style}5. ìµœê·¼ 7ì¼ ì¼ë³„ ì…ê³ /ì¶œê³  í˜„í™©</h3>", unsafe_allow_html=True)

    latest_purchase_date = get_latest_date_from_log(PURCHASE_FILE_ID, PURCHASE_LOG_SHEET_NAME, PURCHASE_DATE_COL, "ì…ê³ ë‚´ì—­.xlsx") # ìš©ì–´ ë³€ê²½
    latest_sales_date = get_latest_date_from_log(SALES_FILE_ID, SALES_LOG_SHEET_NAME, SALES_DATE_COL, "ì¶œê³ ë‚´ì—­.xlsx")       # ìš©ì–´ ë³€ê²½

    overall_latest_date = None
    if latest_purchase_date and latest_sales_date: overall_latest_date = max(latest_purchase_date, latest_sales_date)
    elif latest_purchase_date: overall_latest_date = latest_purchase_date
    elif latest_sales_date: overall_latest_date = latest_sales_date

    if overall_latest_date:
        end_date_7day = overall_latest_date
        start_date_7day = end_date_7day - datetime.timedelta(days=6)
        period_caption = f"ê¸°ê°„: {start_date_7day.strftime('%Y-%m-%d')} ~ {end_date_7day.strftime('%Y-%m-%d')}"
        actual_7day_date_range = [start_date_7day + datetime.timedelta(days=i) for i in range(7)] # datetime.date ê°ì²´ ë¦¬ìŠ¤íŠ¸

        log_cols = st.columns(2)
        with log_cols[0]:
            st.markdown("<h4 style='font-size:1.0rem; margin-bottom:0.1rem;'>ì¼ë³„ ì…ê³  í˜„í™© (Box/Kg)</h4>", unsafe_allow_html=True) # ìš©ì–´ ë³€ê²½
            st.caption(period_caption)
            df_purchase_daily_raw = load_daily_log_data_for_period_from_excel(
                PURCHASE_FILE_ID, PURCHASE_LOG_SHEET_NAME,
                PURCHASE_DATE_COL, PURCHASE_LOCATION_COL, PURCHASE_QTY_BOX_COL, PURCHASE_QTY_KG_COL,
                start_date_7day, end_date_7day,
                is_purchase_log=True, file_name_for_error_msg="ì…ê³ ë‚´ì—­.xlsx" # ìš©ì–´ ë³€ê²½
            )

            if df_purchase_daily_raw is not None and not df_purchase_daily_raw.empty:
                purchase_pivot_box = df_purchase_daily_raw.pivot_table(index=PURCHASE_LOCATION_COL, columns='ë‚ ì§œ', values='TotalQtyBox', fill_value=0)
                purchase_pivot_kg = df_purchase_daily_raw.pivot_table(index=PURCHASE_LOCATION_COL, columns='ë‚ ì§œ', values='TotalQtyKg', fill_value=0)
                purchase_pivot_box = purchase_pivot_box.reindex(index=SUMMARY_TABLE_LOCATIONS, columns=actual_7day_date_range, fill_value=0)
                purchase_pivot_kg = purchase_pivot_kg.reindex(index=SUMMARY_TABLE_LOCATIONS, columns=actual_7day_date_range, fill_value=0)

                purchase_combined_table = pd.DataFrame(index=purchase_pivot_box.index, columns=purchase_pivot_box.columns, dtype=object) # ì»¬ëŸ¼ì€ datetime.date ê°ì²´
                daily_purchase_totals_box = purchase_pivot_box.sum(axis=0)
                daily_purchase_totals_kg = purchase_pivot_kg.sum(axis=0)

                for date_col_obj in purchase_combined_table.columns: # date_col_objëŠ” datetime.date ê°ì²´
                    for loc in purchase_combined_table.index:
                        box = purchase_pivot_box.loc[loc, date_col_obj]
                        kg = purchase_pivot_kg.loc[loc, date_col_obj]
                        purchase_combined_table.loc[loc, date_col_obj] = f"{box:,.0f} / {kg:,.1f}" if not (box == 0 and kg == 0) else "-"

                total_row_data_p = {date_obj: f"{daily_purchase_totals_box.get(date_obj, 0):,.0f} / {daily_purchase_totals_kg.get(date_obj, 0):,.1f}"
                                    if not (daily_purchase_totals_box.get(date_obj, 0) == 0 and daily_purchase_totals_kg.get(date_obj, 0) == 0) else "-"
                                    for date_obj in purchase_combined_table.columns}
                purchase_combined_table.loc['í•©ê³„'] = pd.Series(total_row_data_p)
                # ë‚ ì§œ ì»¬ëŸ¼ì— ìš”ì¼ ì¶”ê°€ (MM/DD(ìš”ì¼) í˜•ì‹)
                purchase_combined_table.columns = [d.strftime('%m/%d') + f"({KOREAN_DAYS[d.weekday()]})" for d in purchase_combined_table.columns]
                st.dataframe(purchase_combined_table.reset_index().rename(columns={'index': 'ì§€ì ëª…'}), hide_index=True, use_container_width=True, height=250)
            else:
                st.write("í•´ë‹¹ ê¸°ê°„ ì…ê³  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.") # ìš©ì–´ ë³€ê²½

        with log_cols[1]:
            st.markdown("<h4 style='font-size:1.0rem; margin-bottom:0.1rem;'>ì¼ë³„ ì¶œê³  í˜„í™© (Box/Kg)</h4>", unsafe_allow_html=True) # ìš©ì–´ ë³€ê²½
            st.caption(period_caption)
            df_sales_daily_raw = load_daily_log_data_for_period_from_excel(
                SALES_FILE_ID, SALES_LOG_SHEET_NAME,
                SALES_DATE_COL, SALES_LOCATION_COL, SALES_QTY_BOX_COL, SALES_QTY_KG_COL,
                start_date_7day, end_date_7day,
                file_name_for_error_msg="ì¶œê³ ë‚´ì—­.xlsx" # ìš©ì–´ ë³€ê²½
            )

            if df_sales_daily_raw is not None and not df_sales_daily_raw.empty:
                sales_pivot_box = df_sales_daily_raw.pivot_table(index=SALES_LOCATION_COL, columns='ë‚ ì§œ', values='TotalQtyBox', fill_value=0)
                sales_pivot_kg = df_sales_daily_raw.pivot_table(index=SALES_LOCATION_COL, columns='ë‚ ì§œ', values='TotalQtyKg', fill_value=0)
                sales_pivot_box = sales_pivot_box.reindex(index=SUMMARY_TABLE_LOCATIONS, columns=actual_7day_date_range, fill_value=0)
                sales_pivot_kg = sales_pivot_kg.reindex(index=SUMMARY_TABLE_LOCATIONS, columns=actual_7day_date_range, fill_value=0)

                sales_combined_table = pd.DataFrame(index=sales_pivot_box.index, columns=sales_pivot_box.columns, dtype=object) # ì»¬ëŸ¼ì€ datetime.date ê°ì²´
                daily_sales_totals_box = sales_pivot_box.sum(axis=0)
                daily_sales_totals_kg = sales_pivot_kg.sum(axis=0)

                for date_col_obj in sales_combined_table.columns: # date_col_objëŠ” datetime.date ê°ì²´
                    for loc in sales_combined_table.index:
                        box = sales_pivot_box.loc[loc, date_col_obj]
                        kg = sales_pivot_kg.loc[loc, date_col_obj]
                        sales_combined_table.loc[loc, date_col_obj] = f"{box:,.0f} / {kg:,.1f}" if not (box == 0 and kg == 0) else "-"

                total_row_data_s = {date_obj: f"{daily_sales_totals_box.get(date_obj, 0):,.0f} / {daily_sales_totals_kg.get(date_obj, 0):,.1f}"
                                    if not (daily_sales_totals_box.get(date_obj, 0) == 0 and daily_sales_totals_kg.get(date_obj, 0) == 0) else "-"
                                    for date_obj in sales_combined_table.columns}
                sales_combined_table.loc['í•©ê³„'] = pd.Series(total_row_data_s)
                # ë‚ ì§œ ì»¬ëŸ¼ì— ìš”ì¼ ì¶”ê°€ (MM/DD(ìš”ì¼) í˜•ì‹)
                sales_combined_table.columns = [d.strftime('%m/%d') + f"({KOREAN_DAYS[d.weekday()]})" for d in sales_combined_table.columns]
                st.dataframe(sales_combined_table.reset_index().rename(columns={'index': 'ì§€ì ëª…'}), hide_index=True, use_container_width=True, height=250)
            else:
                st.write("í•´ë‹¹ ê¸°ê°„ ì¶œê³  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.") # ìš©ì–´ ë³€ê²½
    else:
        st.write("ì…ê³ /ì¶œê³  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ìµœì‹  ë‚ ì§œ ì •ë³´ ì—†ìŒ).") # ìš©ì–´ ë³€ê²½

    st.markdown("---")

    # --- ë„¤ ë²ˆì§¸ í–‰ (ì¹¸ 6): ì „ë…„ ë™ê¸° ì¤‘ëŸ‰ ë¹„êµ (ê°€ë¡œ ì „ì²´) ---
    st.markdown(f"{title_style}6. ì „ë…„ ë™ê¸° ì¤‘ëŸ‰ ë¹„êµ (Kg)</h3>", unsafe_allow_html=True)
    today = datetime.date.today()
    current_year_start = today.replace(month=1, day=1); current_year_end = today
    previous_year_start = current_year_start - relativedelta(years=1); previous_year_end = current_year_end - relativedelta(years=1)
    st.caption(f"ê¸°ê°„: ì˜¬í•´({current_year_start.strftime('%y/%m/%d')}~{current_year_end.strftime('%y/%m/%d')}) vs ì‘ë…„({previous_year_start.strftime('%y/%m/%d')}~{previous_year_end.strftime('%y/%m/%d')})")

    df_sales_cy = load_log_data_for_period_from_excel(SALES_FILE_ID, SALES_LOG_SHEET_NAME, SALES_DATE_COL, SALES_QTY_KG_COL, SALES_LOCATION_COL, current_year_start, current_year_end, file_name_for_error_msg="ì¶œê³ ë‚´ì—­.xlsx") # ìš©ì–´ ë³€ê²½
    df_sales_py = load_log_data_for_period_from_excel(SALES_FILE_ID, SALES_LOG_SHEET_NAME, SALES_DATE_COL, SALES_QTY_KG_COL, SALES_LOCATION_COL, previous_year_start, previous_year_end, file_name_for_error_msg="ì¶œê³ ë‚´ì—­.xlsx") # ìš©ì–´ ë³€ê²½
    df_purchase_cy = load_log_data_for_period_from_excel(PURCHASE_FILE_ID, PURCHASE_LOG_SHEET_NAME, PURCHASE_DATE_COL, PURCHASE_QTY_KG_COL, PURCHASE_LOCATION_COL, current_year_start, current_year_end, is_purchase_log=True, file_name_for_error_msg="ì…ê³ ë‚´ì—­.xlsx") # ìš©ì–´ ë³€ê²½
    df_purchase_py = load_log_data_for_period_from_excel(PURCHASE_FILE_ID, PURCHASE_LOG_SHEET_NAME, PURCHASE_DATE_COL, PURCHASE_QTY_KG_COL, PURCHASE_LOCATION_COL, previous_year_start, previous_year_end, is_purchase_log=True, file_name_for_error_msg="ì…ê³ ë‚´ì—­.xlsx") # ìš©ì–´ ë³€ê²½

    def prepare_comparison_df(df_cy, df_py, name_prefix): # name_prefixì— "ì…ê³ " ë˜ëŠ” "ì¶œê³ " ì „ë‹¬
        df_list = []
        if df_cy is not None and not df_cy.empty:
            df_cy_copy = df_cy.copy(); df_cy_copy['êµ¬ë¶„'] = f'{name_prefix} (ì˜¬í•´)'; df_list.append(df_cy_copy)
        if df_py is not None and not df_py.empty:
            df_py_copy = df_py.copy()
            df_py_copy['ì›”'] = pd.to_datetime(df_py_copy['ì›”']).apply(lambda x: x.replace(year=today.year)).dt.strftime('%Y-%m')
            df_py_copy['êµ¬ë¶„'] = f'{name_prefix} (ì‘ë…„)'; df_list.append(df_py_copy)

        if not df_list: return pd.DataFrame(columns=['ì›”', 'ì¤‘ëŸ‰(Kg)', 'êµ¬ë¶„'])

        combined = pd.concat(df_list)
        return combined

    def plot_comparison_chart(df_combined, title): # titleì— "ì›”ë³„ ì…ê³  ì¤‘ëŸ‰ ë¹„êµ" ë˜ëŠ” "ì›”ë³„ ì¶œê³  ì¤‘ëŸ‰ ë¹„êµ" ì „ë‹¬
        if df_combined.empty or 'ì¤‘ëŸ‰(Kg)' not in df_combined.columns or df_combined['ì¤‘ëŸ‰(Kg)'].sum() == 0 :
            st.write(f"{title}: í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return

        df_combined_sorted = df_combined.copy()
        df_combined_sorted['ì›”_dt'] = pd.to_datetime(df_combined_sorted['ì›”'])
        df_combined_sorted = df_combined_sorted.sort_values('ì›”_dt')

        fig = px.line(df_combined_sorted, x='ì›”', y='ì¤‘ëŸ‰(Kg)', color='êµ¬ë¶„', markers=True,
                        title=title, labels={'ì›”': 'ì›”', 'ì¤‘ëŸ‰(Kg)': 'ì´ ì¤‘ëŸ‰(Kg)'})
        fig.update_layout(height=280, margin=dict(t=30, b=30, l=0, r=0), legend_title_text='')
        st.plotly_chart(fig, use_container_width=True)

    comparison_cols = st.columns(2)
    with comparison_cols[0]:
        df_purchase_compare = prepare_comparison_df(df_purchase_cy, df_purchase_py, "ì…ê³ ") # ìš©ì–´ ë³€ê²½
        plot_comparison_chart(df_purchase_compare, "ì›”ë³„ ì…ê³  ì¤‘ëŸ‰ ë¹„êµ") # ìš©ì–´ ë³€ê²½

    with comparison_cols[1]:
        df_sales_compare = prepare_comparison_df(df_sales_cy, df_sales_py, "ì¶œê³ ") # ìš©ì–´ ë³€ê²½
        plot_comparison_chart(df_sales_compare, "ì›”ë³„ ì¶œê³  ì¤‘ëŸ‰ ë¹„êµ") # ìš©ì–´ ë³€ê²½

# --- ì•± ì‹¤í–‰ ë¶€ë¶„ ---
# SERVICE_ACCOUNT_LOADED ì™€ drive_service ê°€ ì •ìƒì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸ (ë””ë²„ê¹… ì½”ë“œì—ì„œ ì´ë¯¸ ì²˜ë¦¬í•˜ì§€ë§Œ, ìµœì¢… ê´€ë¬¸)
if SERVICE_ACCOUNT_LOADED and drive_service is not None:
    st.success("ìµœì¢… ì¸ì¦ í™•ì¸: Google Drive ì„œë¹„ìŠ¤ ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ. í˜ì´ì§€ ë Œë”ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    render_daily_trend_page_layout()
    st.sidebar.markdown("---")
else:
    # ì´ ë¶€ë¶„ì€ ë””ë²„ê¹… ì½”ë“œì—ì„œ st.stop()ì„ ë§Œë‚˜ì§€ ì•Šì•˜ì§€ë§Œ, ì—¬ì „íˆ ì¸ì¦ì— ì‹¤íŒ¨í•œ ê²½ìš°ë¥¼ ëŒ€ë¹„
    st.error("ìµœì¢… ì¸ì¦ í™•ì¸ ì‹¤íŒ¨: í˜ì´ì§€ë¥¼ ë Œë”ë§í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•± ìƒë‹¨ì˜ ë””ë²„ê¹… ë©”ì‹œì§€ë‚˜ Streamlit Cloud ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    # ì´ë¯¸ critical_auth_error_displayed ì„¸ì…˜ ìƒíƒœê°€ ì„¤ì •ë˜ì—ˆì„ ê²ƒì´ë¯€ë¡œ ì¶”ê°€ ì„¤ì • ë¶ˆí•„ìš”
    pass
